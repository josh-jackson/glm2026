[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Psychological Statistics",
    "section": "",
    "text": "Welcome to the first day of class! If you want to follow along with examples, head over to github for all of the code and data. https://github.com/josh-jackson/glm_2026"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "What are models?\nGLM basics\nGLM basics - 2\nGLM basics - 3\nInteractions\nInteractions - 2\nReview\nDiagnostics\nPower and poly\nMLM\nSEM\nlogistic\nBayes\nMachine learning 1\nMachine learning 2"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "homework",
    "section": "",
    "text": "R Refresher\nHW #2\nHW #3\nHW #4\nHW #5\nHW #6/7\nHW #8\nHW #9"
  },
  {
    "objectID": "R-Refresher.html",
    "href": "R-Refresher.html",
    "title": "R-Refresher",
    "section": "",
    "text": "The activity below is a way for us to get re-acquainted with R. You are welcome to work with others in class. You will give it a shot, and then as a class we’ll come back and work through each part.\nThose of you who took the Applied Statistical Analysis with R class might remember this dataset from your final project. These are data from the Human Connectome Project (HCP), however they have been pared down to make it a little more cohesive. Let’s get going.\nEveryone needs to import the dataset. https://raw.githubusercontent.com/josh-jackson/glm2026/refs/heads/main/hcp-data.csv\ntest\n\n\nGet the mean and standard deviation of the Card Sorting task, PMAT 24 task, and List Sorting task per combination of age category and gender category. Use tidyverse so that you are not calculating all of these manually.\n\n\n\nMake the following 3 figures on any of the variables of your choosing (they do not have to be the same variable(s) per figure):\n\nOverlapping density distribution 1 variable per level of a categorical variable\nScatter plot with a best fit line\nA combination of violin plot and box plot (raw dat points optional)\n\nAll figures must have axes lables that are not the original variable name (aka, make them more readable). Colors should be chosen by you and not the default colors. You must change the theme in at least 1 of the 3 figures.\n\n\n\nTo calculate a correlation by hand, the formula looks like this: \\[ \\frac{\\Sigma z_x z_y}{n-1}\\]\nThat is, you take the \\(z\\)-scores of variable \\(x\\) times the \\(z\\)-scores of variable \\(y\\), and add up those cross products. Then divide that sum of cross products by \\(n-1\\). Pick 2 variables and do this for yourself. After, use the \\(cor\\) function to see if your hand calculation matches the function’s calculation."
  },
  {
    "objectID": "R-Refresher.html#what-is-this",
    "href": "R-Refresher.html#what-is-this",
    "title": "R-Refresher",
    "section": "",
    "text": "The activity below is a way for us to get re-acquainted with R. You are welcome to work with others in class. You will give it a shot, and then as a class we’ll come back and work through each part.\nThose of you who took the Applied Statistical Analysis with R class might remember this dataset from your final project. These are data from the Human Connectome Project (HCP), however they have been pared down to make it a little more cohesive. Let’s get going.\nEveryone needs to import the dataset. https://raw.githubusercontent.com/josh-jackson/glm2026/refs/heads/main/hcp-data.csv\ntest\n\n\nGet the mean and standard deviation of the Card Sorting task, PMAT 24 task, and List Sorting task per combination of age category and gender category. Use tidyverse so that you are not calculating all of these manually.\n\n\n\nMake the following 3 figures on any of the variables of your choosing (they do not have to be the same variable(s) per figure):\n\nOverlapping density distribution 1 variable per level of a categorical variable\nScatter plot with a best fit line\nA combination of violin plot and box plot (raw dat points optional)\n\nAll figures must have axes lables that are not the original variable name (aka, make them more readable). Colors should be chosen by you and not the default colors. You must change the theme in at least 1 of the 3 figures.\n\n\n\nTo calculate a correlation by hand, the formula looks like this: \\[ \\frac{\\Sigma z_x z_y}{n-1}\\]\nThat is, you take the \\(z\\)-scores of variable \\(x\\) times the \\(z\\)-scores of variable \\(y\\), and add up those cross products. Then divide that sum of cross products by \\(n-1\\). Pick 2 variables and do this for yourself. After, use the \\(cor\\) function to see if your hand calculation matches the function’s calculation."
  },
  {
    "objectID": "1-intro.html#glm",
    "href": "1-intro.html#glm",
    "title": "What are models?",
    "section": "GLM",
    "text": "GLM\n\nGeneral(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-generalized",
    "href": "1-intro.html#what-do-we-mean-by-generalized",
    "title": "What are models?",
    "section": "What do we mean by General(ized)?",
    "text": "What do we mean by General(ized)?\n\nIt is general in that it refers to a broad set of similar models that can applied to almost any context"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-linear",
    "href": "1-intro.html#what-do-we-mean-by-linear",
    "title": "What are models?",
    "section": "What do we mean by linear?",
    "text": "What do we mean by linear?\n\nWe try to understand our dependent variable (DV) via a linear combination predictor variables.\nA linear combination a way of combining things (variables) using scalar multiplication and addition"
  },
  {
    "objectID": "1-intro.html#what-is-a-model",
    "href": "1-intro.html#what-is-a-model",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?"
  },
  {
    "objectID": "1-intro.html#what-is-a-model-1",
    "href": "1-intro.html#what-is-a-model-1",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?\n\na representation of the world\na statistical model uses math to make predictions about the world"
  },
  {
    "objectID": "1-intro.html#middle-school-math",
    "href": "1-intro.html#middle-school-math",
    "title": "What are models?",
    "section": "Middle School Math",
    "text": "Middle School Math\n\\[ y = mx + b \\] - what is \\(y\\)?\n\nwhat is \\(m\\)?\nwhat is \\(x\\)?\nwhat is \\(b\\)?"
  },
  {
    "objectID": "1-intro.html#lets-rewrite-this",
    "href": "1-intro.html#lets-rewrite-this",
    "title": "What are models?",
    "section": "Let’s rewrite this",
    "text": "Let’s rewrite this\n\\[y = b_0 + b_{1}X\\]\n\nwhat is \\(y\\)?\nwhat is \\(b_0\\)?\nwhat is \\(b_1\\)?\nwhat is \\(X\\)?"
  },
  {
    "objectID": "1-intro.html#are-models-always-right",
    "href": "1-intro.html#are-models-always-right",
    "title": "What are models?",
    "section": "Are models always right?",
    "text": "Are models always right?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed",
    "href": "1-intro.html#models-are-flawed",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed-1",
    "href": "1-intro.html#models-are-flawed-1",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "1-intro.html#models",
    "href": "1-intro.html#models",
    "title": "What are models?",
    "section": "Models",
    "text": "Models\n\nWhat are the goals of modeling?\nWhat do you need in order to develop a model?"
  },
  {
    "objectID": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "href": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "title": "What are models?",
    "section": "How do we know if a model is good?",
    "text": "How do we know if a model is good?\n\nWhat makes it good?"
  },
  {
    "objectID": "1-intro.html#how-will-we-use-models",
    "href": "1-intro.html#how-will-we-use-models",
    "title": "What are models?",
    "section": "How will we use models?",
    "text": "How will we use models?\n\nThis semester, we will mainly focus on classic statistical tests\nEvery single one of these is a model\nWe will also focus on developing your intuition\nWhen you face new models, come back to these basics"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models",
    "href": "2-Basics.html#thinking-in-terms-of-models",
    "title": "correlation",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "href": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "title": "correlation",
    "section": "How can we visualize data to make sense of it?",
    "text": "How can we visualize data to make sense of it?\n\n\nCode\nlibrary(broom)\nset.seed(123)\nx.1 &lt;- rnorm(100, 0, 1)\ne.1 &lt;- rnorm(100, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\nd.1\n\n\n             x.1         y.1\n1   -0.560475647 -1.22907473\n2   -0.230177489  0.88716980\n3    1.558708314  0.86390582\n4    0.070508391 -0.15630558\n5    0.129287735 -1.33212888\n6    1.715064987  1.35323029\n7    0.460916206 -0.81630503\n8   -1.265061235 -3.53166755\n9   -0.686852852 -0.63822211\n10  -0.445661970  2.09287913\n11   1.224081797  0.02255106\n12   0.359813827  1.91382625\n13   0.400771451 -2.51534112\n14   0.110682716  0.44975156\n15  -0.555841135  1.23310178\n16   1.786913137  2.08510895\n17   0.497850478  0.98517015\n18  -1.966617157 -1.86305145\n19   0.701355902 -0.81366295\n20  -0.472791408 -1.80829286\n21  -1.067823706  0.14799016\n22  -0.217974915 -1.51483543\n23  -1.026004448 -1.04541733\n24  -0.728891229 -0.41307456\n25  -0.625039268  3.84395241\n26  -1.686693311 -1.73158112\n27   0.837787044  1.43155602\n28   0.153373118  0.74027691\n29  -1.138136937 -2.04968858\n30   1.253814921  1.04698203\n31   0.426464221  3.62365704\n32  -0.295071483  1.24071879\n33   0.895125661  1.07478496\n34   0.878133488  0.13797975\n35   0.821581082 -3.15462485\n36   0.688640254  3.14142657\n37   0.553917654 -2.11662543\n38  -0.061911711  1.94584358\n39  -0.305962664  4.14992767\n40  -0.380471001 -2.59704537\n41  -0.694706979  1.52147983\n42  -0.207917278 -0.13874948\n43  -1.265396352 -3.34025631\n44   2.168955965 -1.33640953\n45   1.207961998 -2.03869325\n46  -1.123108583 -1.17952277\n47  -0.402884835 -2.64509783\n48  -0.466655354  1.61917310\n49   0.779965118  5.12919870\n50  -0.083369066 -2.11991394\n51   0.253318514  2.21480288\n52  -0.028546755  2.02238377\n53  -0.042870457  1.14082641\n54   1.368602284 -0.76402196\n55  -0.225770986  0.13692074\n56   1.516470604  0.77326816\n57  -1.548752804  0.77416502\n58   0.584613750  0.07666005\n59   0.123854244  2.52206661\n60   0.215941569 -0.13039385\n61   0.379639483  2.81422465\n62  -0.502323453 -1.87463191\n63  -0.333207384 -2.20357455\n64  -1.018575383  6.42186341\n65  -1.071791226 -0.92320035\n66   0.303528641  1.26339594\n67   0.448209779  2.01965473\n68   0.053004227 -0.43840893\n69   0.922267468  2.04097120\n70   2.050084686  2.36547563\n71  -0.491031166 -0.20082816\n72  -2.309168876 -0.63945681\n73   1.005738524  0.98502168\n74  -0.709200763  4.36684338\n75  -0.688008616 -1.36107693\n76   1.025571370 -1.12792828\n77  -0.284773007  0.41895164\n78  -1.220717712  0.44956676\n79   0.181303480  1.47276387\n80  -0.138891362 -0.49312091\n81   0.005764186 -1.62348197\n82   0.385280401  3.23827457\n83  -0.370660032 -0.40316379\n84   0.644376549 -0.87661862\n85  -0.220486562 -0.09382675\n86   0.331781964  0.28812829\n87   1.096839013  3.32310204\n88   0.435181491  0.90882440\n89  -0.325931586  1.82884520\n90   1.148807618  0.13326016\n91   0.993503856  1.47531774\n92   0.548396960  0.15224650\n93   0.238731735  0.82046951\n94  -0.627906076 -1.63607506\n95   1.360652449 -1.37324422\n96  -0.600259587  4.16428400\n97   2.187332993  2.90445079\n98   1.532610626 -1.15960688\n99  -0.235700359 -0.85196703\n100 -1.026420900 -2.43549166"
  },
  {
    "objectID": "2-Basics.html#how-do-we-visualize-categorical-data",
    "href": "2-Basics.html#how-do-we-visualize-categorical-data",
    "title": "correlation",
    "section": "How do we visualize categorical data?",
    "text": "How do we visualize categorical data?\nNominal/categorical data does not have any inherent numbers associated with it. Think control/tx, eye color, etc.\n\n\nCode\nset.seed(123)\ngroup &lt;- c(0, 1)\nx.2 &lt;- rep(group, times = 50)\ne.1 &lt;- rnorm(100, 0, 1)\ny.1 &lt;- .5 + .85 * x.2 + e.1\nd.2 &lt;- data.frame(x.2,y.1)\nm.2 &lt;- lm(y.1 ~ x.2, data = d.2)\nd2.f&lt;- augment(m.2)\nd.2\n\n\n    x.2          y.1\n1     0 -0.060475647\n2     1  1.119822511\n3     0  2.058708314\n4     1  1.420508391\n5     0  0.629287735\n6     1  3.065064987\n7     0  0.960916206\n8     1  0.084938765\n9     0 -0.186852852\n10    1  0.904338030\n11    0  1.724081797\n12    1  1.709813827\n13    0  0.900771451\n14    1  1.460682716\n15    0 -0.055841135\n16    1  3.136913137\n17    0  0.997850478\n18    1 -0.616617157\n19    0  1.201355902\n20    1  0.877208592\n21    0 -0.567823706\n22    1  1.132025085\n23    0 -0.526004448\n24    1  0.621108771\n25    0 -0.125039268\n26    1 -0.336693311\n27    0  1.337787044\n28    1  1.503373118\n29    0 -0.638136937\n30    1  2.603814921\n31    0  0.926464221\n32    1  1.054928517\n33    0  1.395125661\n34    1  2.228133488\n35    0  1.321581082\n36    1  2.038640254\n37    0  1.053917654\n38    1  1.288088289\n39    0  0.194037336\n40    1  0.969528999\n41    0 -0.194706979\n42    1  1.142082722\n43    0 -0.765396352\n44    1  3.518955965\n45    0  1.707961998\n46    1  0.226891417\n47    0  0.097115165\n48    1  0.883344646\n49    0  1.279965118\n50    1  1.266630934\n51    0  0.753318514\n52    1  1.321453245\n53    0  0.457129543\n54    1  2.718602284\n55    0  0.274229014\n56    1  2.866470604\n57    0 -1.048752804\n58    1  1.934613750\n59    0  0.623854244\n60    1  1.565941569\n61    0  0.879639483\n62    1  0.847676547\n63    0  0.166792616\n64    1  0.331424617\n65    0 -0.571791226\n66    1  1.653528641\n67    0  0.948209779\n68    1  1.403004227\n69    0  1.422267468\n70    1  3.400084686\n71    0  0.008968834\n72    1 -0.959168876\n73    0  1.505738524\n74    1  0.640799237\n75    0 -0.188008616\n76    1  2.375571370\n77    0  0.215226993\n78    1  0.129282288\n79    0  0.681303480\n80    1  1.211108638\n81    0  0.505764186\n82    1  1.735280401\n83    0  0.129339968\n84    1  1.994376549\n85    0  0.279513438\n86    1  1.681781964\n87    0  1.596839013\n88    1  1.785181491\n89    0  0.174068414\n90    1  2.498807618\n91    0  1.493503856\n92    1  1.898396960\n93    0  0.738731735\n94    1  0.722093924\n95    0  1.860652449\n96    1  0.749740413\n97    0  2.687332993\n98    1  2.882610626\n99    0  0.264299641\n100   1  0.323579100"
  },
  {
    "objectID": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "href": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "title": "correlation",
    "section": "What do these visualizations have in common?",
    "text": "What do these visualizations have in common?\n\nLINES!\nMost of what we are going to do is represent the relationship between variables with lines (or planes or hyperplanes once we get into 2 or more variables)"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models-1",
    "href": "2-Basics.html#thinking-in-terms-of-models-1",
    "title": "correlation",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nModels help us draw the lines\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (hence forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)\n\n\\(b_{1}\\) describes the strength of association i.e. the line!"
  },
  {
    "objectID": "2-Basics.html#see-this-in-our-r-code",
    "href": "2-Basics.html#see-this-in-our-r-code",
    "title": "correlation",
    "section": "See this in our R code",
    "text": "See this in our R code\nIndependent samples t-test\n\n\nCode\nt.1 &lt;- t.test(y.1 ~ x.2, var.equal = TRUE, data = d.2) \nt.1\n\n\n\n    Two Sample t-test\n\ndata:  y.1 by x.2\nt = -4.4144, df = 98, p-value = 2.607e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.1738643 -0.4457736\nsample estimates:\nmean in group 0 mean in group 1 \n      0.6104964       1.4203154"
  },
  {
    "objectID": "2-Basics.html#comparison-across-these-three-models",
    "href": "2-Basics.html#comparison-across-these-three-models",
    "title": "correlation",
    "section": "Comparison across these three models",
    "text": "Comparison across these three models\n\nNote that each of these three models (t, anova, regression) were exactly the same in terms of the mode: Y ~ X\nThat is because they are the same model! Different terms referring to the same thing is one of the major stumbling blocks of stats.\nYet they gave us different information. Depending on what you are interested in some information may be more pertinent.\nWe will focus on the regression model (glm) as it is most flexible"
  },
  {
    "objectID": "2-Basics.html#general-linear-model-glm",
    "href": "2-Basics.html#general-linear-model-glm",
    "title": "correlation",
    "section": "General linear model (GLM)",
    "text": "General linear model (GLM)\n\nThis model (equation) can be very simple as in a treatment/control experiment\nIt can be very complex in terms of trying to understand something like academic achievement\nThe majority of our models fall under the umbrella of a general(ized) linear model (often referred to as regression models)\nModels imply our theory about how the data are generated (ie how the world works)"
  },
  {
    "objectID": "2-Basics.html#regression-equation",
    "href": "2-Basics.html#regression-equation",
    "title": "correlation",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[Y_i = b_{0} + b_{1}X_i +e_i\\]\n\n\\(Y_i \\sim Normal(\\mu, \\sigma)\\)\nThe DV, \\(Y\\) is assumed to be distributed as a Gaussian normal, made up of \\(Y_i\\), with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)"
  },
  {
    "objectID": "2-Basics.html#regression-terms",
    "href": "2-Basics.html#regression-terms",
    "title": "correlation",
    "section": "Regression terms",
    "text": "Regression terms\n\nY / DV / Outcome / Response / Criterion\nX / IV / Predictor / Explanatory variable\nRegression coefficient (weight) / b / b* / \\(\\beta\\)\nIntercept \\(b_0\\) / \\(\\beta_{0}\\)\nError / Residuals \\(e\\)\nPredictions \\(\\hat{Y}\\)"
  },
  {
    "objectID": "2-Basics.html#regression-models",
    "href": "2-Basics.html#regression-models",
    "title": "correlation",
    "section": "Regression models",
    "text": "Regression models\n\nThese models are a way to convey the relationship between two (or more) variables. They translate our hypotheses into math.\nWe can use these models to get information we may be interested in (e.g. means, SEs) and test hypotheses about the relationship among variables\n“All models are wrong but some are useful (and some are better than others)” - George Box"
  },
  {
    "objectID": "2-Basics.html#another-example",
    "href": "2-Basics.html#another-example",
    "title": "correlation",
    "section": "Another example",
    "text": "Another example\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nexample.data &lt;- read_csv(\"exampleData.csv\")\nexample.data &lt;- na.omit(example.data)\nexample.data\n\n\n# A tibble: 270 × 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#example-summary",
    "href": "2-Basics.html#example-summary",
    "title": "correlation",
    "section": "Example summary",
    "text": "Example summary\n\nSame p-values for each test; same SS; same test!\nt-test is a special form of a linear model\nanova is a special form of a linear model\nBecause the anova and t-test are narrower models, we will be working with the general linear model"
  },
  {
    "objectID": "2-Basics.html#parts-of-the-model",
    "href": "2-Basics.html#parts-of-the-model",
    "title": "correlation",
    "section": "Parts of the model",
    "text": "Parts of the model\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\] \\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\nEach individual has a unique Y value an X value and a residual/error term\n\nThe model only has a single \\(b_{0}\\) and \\(b_{1}\\) term. These are the regression parameters. \\(b_{0}\\) is the intercept and \\(b_{1}\\) quantifies the relationship between your model of the world and the DV."
  },
  {
    "objectID": "2-Basics.html#what-do-the-estimates-tell-us",
    "href": "2-Basics.html#what-do-the-estimates-tell-us",
    "title": "correlation",
    "section": "What do the estimates tell us?",
    "text": "What do the estimates tell us?\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nlibrary(broom)\ntidy(mod.1)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates",
    "href": "2-Basics.html#how-to-interpret-regression-estimates",
    "title": "correlation",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nIntercept is the mean of group of variable tx that is coded 0\nRegression coefficient is the slope or rise over run, scaled as a 1 unit on the x axis\n“For a one unit change in X, there is a b1 predicted change in Y.”\nRegression coefficient is the difference in means between the groups, given that we coded our groups as 0 and 1."
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "href": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "title": "correlation",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nThe entire class will go over different ways to interpret these estimates/parameters/coefficients\nIntercept (b0) signifies the level of Y when your model IVs (Xs) are zero\nRegression (b1) signifies the difference for a one unit change in your X"
  },
  {
    "objectID": "2-Basics.html#standard-errors",
    "href": "2-Basics.html#standard-errors",
    "title": "correlation",
    "section": "Standard errors",
    "text": "Standard errors\n\nThese coefficients are “best guesses” at some population parameter we want to make inferences about.\nTo do so we must balance our signal to our noise. If we have a strong signal (steep regression line/difference between groups) that would imply the groups differ.\nIf there was a lot of noise in that assessment then a big difference between groups may not be meaningful. We assess this “noise” component with our standard errors"
  },
  {
    "objectID": "2-Basics.html#sampling-distribution-refresher",
    "href": "2-Basics.html#sampling-distribution-refresher",
    "title": "correlation",
    "section": "Sampling distribution refresher",
    "text": "Sampling distribution refresher\n\nWe collect a sample and calculate a sample statistic \\(b_1\\)\nThis statistic is not a perfect assessment of the population\nWe calculate a sampling distribution to represent all possible samples we could have gotten from the same population with the same sample size\nThe standard deviation of the sampling distribution (standard error) reflects the spread of the hypothetical scores we could have gotten\nA large standard error means we have a flat sampling distribution and thus should not trust the estimate.\nPer convention, if our estimate is &gt; 2xSE away from 0, we say our estimate is “significantly different from zero”"
  },
  {
    "objectID": "2-Basics.html#predicted-scores",
    "href": "2-Basics.html#predicted-scores",
    "title": "correlation",
    "section": "Predicted scores",
    "text": "Predicted scores\n\nBased on the output how do I calculate means for each group?\n\n\ntidy(mod.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#anova-as-regression",
    "href": "2-Basics.html#anova-as-regression",
    "title": "correlation",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\n“For a one unit change in X, there is a b1 predicted change in Y” will always be true.\nNominal/categorical variables do not have any inherent numbers associated with them so we need to assign them numbers\nWhat numbers you assign will impact the equation/estimates/hypothesis you can test\n\nBehoove you to code them as useful numbers. O and 1 are useful and are the default in R."
  },
  {
    "objectID": "2-Basics.html#anova-as-regression-1",
    "href": "2-Basics.html#anova-as-regression-1",
    "title": "correlation",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "href": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "title": "correlation",
    "section": "What if we changed 0 and 1 to other values?",
    "text": "What if we changed 0 and 1 to other values?\n\nInfinite number of ways to code categorical/nominal variables, only a few meaningful ways\n\nThe R default is called “dummy coding”\n\nUses 0s and 1s to put numbers to categories. We will soon see what this looks like when you have more than 2 groups.\nChanging the numbers changes…?"
  },
  {
    "objectID": "2-Basics.html#effect-coding",
    "href": "2-Basics.html#effect-coding",
    "title": "correlation",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nexample.data$tx.effect &lt;- dplyr::recode(example.data$tx, '0' = -1, '1' = 1) \n\n\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 5\n      id    tx traffic.risk tx.r      tx.effect\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1     1     1         1.86 treatment         1\n 2     2     1         1    treatment         1\n 3     3     1         3.29 treatment         1\n 4     4     1         2    treatment         1\n 5     5     1         2.43 treatment         1\n 6     6     1         3.29 treatment         1\n 7     7     0         1.17 control          -1\n 8     8     0         2.43 control          -1\n 9     9     0         3    control          -1\n10    10     1         1.71 treatment         1\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#effect-coding-1",
    "href": "2-Basics.html#effect-coding-1",
    "title": "correlation",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nmod.1.eff &lt;- lm(traffic.risk ~ tx.effect, data = example.data)\ntidy(mod.1.eff)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.41     0.0487     49.5  8.15e-137\n2 tx.effect     -0.241    0.0487     -4.94 1.38e-  6\n\n\n\nsystematically changes both the intercept and the regression estimate"
  },
  {
    "objectID": "2-Basics.html#effect-coding-2",
    "href": "2-Basics.html#effect-coding-2",
    "title": "correlation",
    "section": "Effect coding",
    "text": "Effect coding\nConsists of -1, 1s (And zeros for more than 2 groups)\n\nThe intercept is the “grand mean” or “mean of means” if unbalanced\nThe regression coefficient represents the group “effect” ie the difference between the grand mean and the group labeled 1 (we will revisit this when we have more than 2 groups as it will make more sense)\n\n\nCommon to use for Factorial ANOVA designs"
  },
  {
    "objectID": "2-Basics.html#dummy-coding",
    "href": "2-Basics.html#dummy-coding",
    "title": "correlation",
    "section": "Dummy coding",
    "text": "Dummy coding\n\nMore appropriate when you are interested in comparing to a specific group rather than an “average person.”\nIntercept: value of the group coded zero\nRegression coefficient: mean difference between groups"
  },
  {
    "objectID": "2-Basics.html#contrast-coding",
    "href": "2-Basics.html#contrast-coding",
    "title": "correlation",
    "section": "Contrast coding",
    "text": "Contrast coding\n\nAs our models get more complex our coding schemes can too\nWhat happens if you code the groups -.5 and .5?\nThese make more sense when we have more groups. More groups require more independent variables, however."
  },
  {
    "objectID": "2-Basics.html#categorical-coding-summary",
    "href": "2-Basics.html#categorical-coding-summary",
    "title": "correlation",
    "section": "Categorical coding summary",
    "text": "Categorical coding summary\n\nIn the end, it really doesn’t matter how you code your model. The overall “fit” of the model will be exactly the same because it is the same model.\nThe only thing that changes is the interpretation of your coefficients.\nEven then, you can recreate any test you want regardless of coding scheme. As a result, we often leave the default coding in place."
  },
  {
    "objectID": "2-Basics.html#statistical-inference",
    "href": "2-Basics.html#statistical-inference",
    "title": "correlation",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nThe way the world is = our model + error\nHow good is our model? Is it a good representation of reality? Does it “fit” the data well?\nNeed to go beyond asking if it is significant, because what does that mean? Remember, all models are wrong\nWe are going to make predictions and see if the predictions (based on our model) matches our data\nWe can then compare one model to another to see which one matches our data better ie which one is a better representation of reality."
  },
  {
    "objectID": "2-Basics.html#predictions",
    "href": "2-Basics.html#predictions",
    "title": "correlation",
    "section": "Predictions",
    "text": "Predictions\n\nOur model is a prediction machine.\nThey are created by simply plugging a persons Xs into the created model\nIf you have bs and have Xs you can create a prediction\n\n\\(\\hat{Y}_{i}\\) = 2.65064 + -0.48111* \\(X_{i}\\)"
  },
  {
    "objectID": "2-Basics.html#predictions-1",
    "href": "2-Basics.html#predictions-1",
    "title": "correlation",
    "section": "Predictions",
    "text": "Predictions\n\nWe want our predictions to be close to our actual data for each person ( \\(Y_{i}\\) )\nThe difference between the actual data and our our prediction ( \\(Y_{i} - \\hat{Y}_{i} = e\\) ) is the residual, how far we are “off”. This tells us how good our fit is.\nYou can have the same estimates for two models but completely different fit.\nPreviously you may have evaluated overall model fit by looking at Eta Squared, SS Error and visualizing observations around group means"
  },
  {
    "objectID": "2-Basics.html#which-one-has-better-fit",
    "href": "2-Basics.html#which-one-has-better-fit",
    "title": "correlation",
    "section": "Which one has better fit?",
    "text": "Which one has better fit?\n\nCan you point out the predictions?\n\n\n\nCode\ntwogroup_fun = function(nrep = 100, b0 = 6, b1 = -2, sigma = 1) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\n\ntwogroup_fun2 = function(nrep = 100, b0 = 6, b1 = -2, sigma = 2) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\nset.seed(16)\nlibrary(broom)\nlm1 &lt;- augment(twogroup_fun())\n\nset.seed(16)\nlm2 &lt;- augment(twogroup_fun2())\n\nplot1&lt;- ggplot(lm1) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\nplot2&lt;- ggplot(lm2) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\n\nlibrary(gridExtra)\n grid.arrange(plot1, plot2, ncol=2)"
  },
  {
    "objectID": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "href": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "title": "correlation",
    "section": "Easy to examine fit with lm objects",
    "text": "Easy to examine fit with lm objects\n\nThese are automatically created anytime you run a lm in R\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\n\n\n\ncoefficients(mod.1)       # coefficients\nresiduals(mod.1)          # residuals\nfitted.values(mod.1)      # fitted values ie predicted\nsummary(mod.1)$r.squared  # R-sq for the model\nsummary(mod.1)$sigma      # sd of residuals"
  },
  {
    "objectID": "2-Basics.html#an-aside-concerning-lm-objects",
    "href": "2-Basics.html#an-aside-concerning-lm-objects",
    "title": "correlation",
    "section": "An aside concerning lm objects",
    "text": "An aside concerning lm objects\n\nlm objects consist of the information embedded in your linear model\nR often handles model objects poorly due to them not necessarily being in a usable data frame (lists!)\nthe broom package makes model objects into dataframes\n\n\n\nCode\nlibrary(broom)\nfit.1.tidy &lt;- tidy(mod.1)  \nfit.1.tidy\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#statistical-inference-1",
    "href": "2-Basics.html#statistical-inference-1",
    "title": "correlation",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well\nSaid differently, the closer our model is to the “actual” data generating model, our guesses ( \\(\\hat{Y}\\) ) will be closer to our actual data ( \\(Y\\) )"
  },
  {
    "objectID": "2-Basics.html#partitioning-the-variation-in-y",
    "href": "2-Basics.html#partitioning-the-variation-in-y",
    "title": "correlation",
    "section": "Partitioning the variation in Y",
    "text": "Partitioning the variation in Y\n\\[ \\sum (Y_i - \\bar{Y})^2 = \\sum (\\hat{Y}_i -\\bar{Y})^2 + \\sum(Y_i - \\hat{Y}_i)^2 \\]\n\nSS total = SS between + SS within\nSS total = SS regression + SS residual (or error)"
  },
  {
    "objectID": "2-Basics.html#what-can-we-do-with-this",
    "href": "2-Basics.html#what-can-we-do-with-this",
    "title": "correlation",
    "section": "What can we do with this?",
    "text": "What can we do with this?\n\nomnibus F tests (ANOVA)\nWhat hypothesis does the omnibus F test test, generally?\n\n\\[s_{y}^2 = s_{regression}^2 + s_{residual}^2\\]\n\\[1 = \\frac{s_{regression}^2}{s_{y}^2} + \\frac{s_{residual}^2}{s_{y}^2}\\]"
  },
  {
    "objectID": "2-Basics.html#coefficient-of-determination",
    "href": "2-Basics.html#coefficient-of-determination",
    "title": "correlation",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\\[\\frac{s_{regression}^2}{s_{y}^2} = \\frac{SS_{regression}}{SS_{Y}} = R^2\\]\n\nPercent (of total) variance explained by your model…which currently are groups\nAnother way of asking how much variance group status explains"
  },
  {
    "objectID": "2-Basics.html#r2-and-eta-squared",
    "href": "2-Basics.html#r2-and-eta-squared",
    "title": "correlation",
    "section": "\\(R^2\\) and Eta squared",
    "text": "\\(R^2\\) and Eta squared\n\nsummary(mod.1)$r.squared\n\n[1] 0.08344007\n\n\n\nlibrary(lsr)\netaSquared(mod.1)\n\n       eta.sq eta.sq.part\ntx 0.08344007  0.08344007"
  },
  {
    "objectID": "2-Basics.html#r2-for-different-coding-schemes",
    "href": "2-Basics.html#r2-for-different-coding-schemes",
    "title": "correlation",
    "section": "\\(R^2\\) for different coding schemes",
    "text": "\\(R^2\\) for different coding schemes\n\nglance(mod.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(mod.1.eff)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "2-Basics.html#note-the-r2-p-value",
    "href": "2-Basics.html#note-the-r2-p-value",
    "title": "correlation",
    "section": "Note the \\(R^2\\) p-value",
    "text": "Note the \\(R^2\\) p-value\n\n\nCode\nsummary(mod.1)\n\n\n\nCall:\nlm(formula = traffic.risk ~ tx, data = example.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65064 -0.59811 -0.02668  0.54475  2.54475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***\ntx          -0.48111    0.09740  -4.939 1.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7789 on 268 degrees of freedom\nMultiple R-squared:  0.08344,   Adjusted R-squared:  0.08002 \nF-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06"
  },
  {
    "objectID": "2-Basics.html#summary",
    "href": "2-Basics.html#summary",
    "title": "correlation",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "2-Basics.html#summary-1",
    "href": "2-Basics.html#summary-1",
    "title": "correlation",
    "section": "Summary",
    "text": "Summary\n\nWe are using linear models to do the exact same tests as t-tests and ANOVAs\nIt is the exact same because t-tests and ANOVAs are part of the general linear model\nThe GLM provides a more systematic way at 1) building and testing your theoretical model and 2) comparing between alternative theoretical models\nYou can get 1) estimates and 2) fit statistics from the model. Both are important."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Download syllabus"
  },
  {
    "objectID": "10-bayes.html#reassurance-before-we-get-to-the-three-steps",
    "href": "10-bayes.html#reassurance-before-we-get-to-the-three-steps",
    "title": "Bayes",
    "section": "Reassurance, before we get to the three steps",
    "text": "Reassurance, before we get to the three steps\n\nNot drastically different!\nYou get to keep everything you like\nYour models stay the same!!"
  },
  {
    "objectID": "10-bayes.html#glm",
    "href": "10-bayes.html#glm",
    "title": "Bayes",
    "section": "GLM",
    "text": "GLM\n\nOur good friend that gives us 99% of the models psychologists use (general(ized) linear model), is exactly the same\n\n\\[\\Large Y = b_{0} + b_{1}X +e\\] - No need to think about setting up new t-test, ANOVAS, regressions, etc. ALL THE SAME."
  },
  {
    "objectID": "10-bayes.html#a-working-mental-model",
    "href": "10-bayes.html#a-working-mental-model",
    "title": "Bayes",
    "section": "A working mental model",
    "text": "A working mental model\nWhat are Bayesian models?\n\n“Normal” regression with a different algorithm.\nResults that represent a distribution rather than a point estimate and some uncertainty.\nPriors that incorporate existing knowledge."
  },
  {
    "objectID": "10-bayes.html#be-comfortable-with-a-different-estimation-algorithm",
    "href": "10-bayes.html#be-comfortable-with-a-different-estimation-algorithm",
    "title": "Bayes",
    "section": "1. Be comfortable with a different estimation algorithm",
    "text": "1. Be comfortable with a different estimation algorithm\n\nWhat do you mean by estimation algorithm?\n\n\n\nOLS i.e. \\(min\\sum(Y_{i}-\\hat{Y})^{2}\\)\nFun fact, R uses QR decomposition, Newton Raphson, Fisher Scoring, SVR, etc – not this equation.\nAnother fun fact, more advanced stats use an even different algorithm (e.g., maximum likelihood)"
  },
  {
    "objectID": "10-bayes.html#standard-way",
    "href": "10-bayes.html#standard-way",
    "title": "Bayes",
    "section": "Standard way",
    "text": "Standard way\n\n\nCode\nlibrary(tidyverse)\ngalton.data &lt;- psychTools::galton"
  },
  {
    "objectID": "10-bayes.html#fisher-scoring",
    "href": "10-bayes.html#fisher-scoring",
    "title": "Bayes",
    "section": "Fisher Scoring",
    "text": "Fisher Scoring\n\n\nCode\nfit.1.g &lt;- glm(child ~ parent, family = gaussian, data = galton.data)\nsummary(fit.1.g)\n\n\n\nCall:\nglm(formula = child ~ parent, family = gaussian, data = galton.data)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5.011094)\n\n    Null deviance: 5877.2  on 927  degrees of freedom\nResidual deviance: 4640.3  on 926  degrees of freedom\nAIC: 4133.2\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "10-bayes.html#maximum-likelihood",
    "href": "10-bayes.html#maximum-likelihood",
    "title": "Bayes",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         3\n\n  Number of observations                           928\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  child ~                                             \n    parent            0.646    0.041   15.728    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .child            23.942    2.808    8.527    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .child             5.000    0.232   21.541    0.000"
  },
  {
    "objectID": "10-bayes.html#bayesian-way",
    "href": "10-bayes.html#bayesian-way",
    "title": "Bayes",
    "section": "Bayesian way",
    "text": "Bayesian way\n\n\nCode\nlibrary(brms)\nfit.1.bayesian &lt;- brm(child ~ parent, data = galton.data, backend = \"cmdstanr\", file = \"fit.1.b\")\n\n\n\nsummary(fit.1.bayesian)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: child ~ parent \n   Data: galton.data (Number of observations: 928) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    23.93      2.88    18.31    29.43 1.00     4035     3188\nparent        0.65      0.04     0.57     0.73 1.00     4034     3161\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.24      0.05     2.14     2.35 1.00     3190     2478\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "10-bayes.html#step-1-is-easy",
    "href": "10-bayes.html#step-1-is-easy",
    "title": "Bayes",
    "section": "Step 1 is easy",
    "text": "Step 1 is easy\n\nBayes gives you basically the same results\n\n\n\nSo why use it? Many reasons, but the most direct is manipulating, visualizing, and extrapolating from results"
  },
  {
    "objectID": "10-bayes.html#think-of-results-in-terms-of-distributions",
    "href": "10-bayes.html#think-of-results-in-terms-of-distributions",
    "title": "Bayes",
    "section": "2. Think of results in terms of distributions",
    "text": "2. Think of results in terms of distributions\n\nWhat are results?\n\n\n\nEstimate and an SE\nIndicates a “best guess” ie mean/median/mode and the imprecision related to it\nIf this guess is far away from zero (and imprecision not large), then it is significant\nWe know that if we repeated this again we won’t get the same answer (estimate), but likely in between our CIs\nHow do we convey the “best guess?”"
  },
  {
    "objectID": "10-bayes.html#posterior-distribution-ie-results",
    "href": "10-bayes.html#posterior-distribution-ie-results",
    "title": "Bayes",
    "section": "Posterior distribution (ie results)",
    "text": "Posterior distribution (ie results)\n\nIs made up of a series of educated guesses (via our algorithm), each of which is consistent with the data.\nIn aggregate, these guesses provide us not with a best guess and an SD (as with Maximum Likelihood), but a more complete sense of each parameter we are trying to estimate.\nWe can assume this distribution (typically normal) with standard estimation, but with bayes it can be flexible!"
  },
  {
    "objectID": "10-bayes.html#posterior-distribution-ie-results-1",
    "href": "10-bayes.html#posterior-distribution-ie-results-1",
    "title": "Bayes",
    "section": "Posterior distribution (ie results)",
    "text": "Posterior distribution (ie results)\nIs made of up of a series of educated guesses. Each dot represents a particular guess. Guesses that occur more often are considered more likely.\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()"
  },
  {
    "objectID": "10-bayes.html#how-does-the-algorithm-work",
    "href": "10-bayes.html#how-does-the-algorithm-work",
    "title": "Bayes",
    "section": "How does the algorithm work?",
    "text": "How does the algorithm work?\n\nPlayed a role in developing the thermonuclear bomb with one of the earliest computers. Published in 1953 but ignored within stats b/c it was published within a physics/chemistry journal. Took about until 1990 for desktop computers to run fast enough to do at home.\nMany variants, but the general idea is a) propose an estimate value + noise N(0, \\(\\sigma\\) ) then b) see how “likely” the data is given the estimate, c) based on some criteria (better than worse that some value) either accept or reject the estimate and d) repeat"
  },
  {
    "objectID": "10-bayes.html#what-do-you-mean-by-likely",
    "href": "10-bayes.html#what-do-you-mean-by-likely",
    "title": "Bayes",
    "section": "What do you mean by likely?",
    "text": "What do you mean by likely?\nYou’ve done this before last semester. Three parameters in a binomial distribution (# successes, # of trials, probability of success). Often you would fix #trials and probability of success to see what # successes are most/least likely.\n\n\nCode\ndata.frame(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = .5)) %&gt;% \n  ggplot(aes(x = factor(heads), y = prob)) +\n  geom_col(fill = \"#562457\") +\n  geom_text(aes(label = round(prob, 2), y = prob + .01),\n            position = position_dodge(.9),\n            size = 5, \n            vjust = 0) +\n  labs(title = \"Binomial Distribution of Coin Flips\",\n       subtitle = \"n = 10, p = .5\",\n       x = \"Number of Successes (Heads)\",\n       y = \"Density\") +\n  theme_classic(base_size = 16)"
  },
  {
    "objectID": "10-bayes.html#more-intuituion",
    "href": "10-bayes.html#more-intuituion",
    "title": "Bayes",
    "section": "More intuituion",
    "text": "More intuituion\n\nThink of the algorithm as picking out marbles from a sack, with replacement, to figure out the distribution of colors.\nOr us doing rnorm with me hiding what the mean and SD are, but then figuring out what the mean and SD are through counting the samples."
  },
  {
    "objectID": "10-bayes.html#bayesian-analysis-is-just-counting",
    "href": "10-bayes.html#bayesian-analysis-is-just-counting",
    "title": "Bayes",
    "section": "Bayesian analysis is just counting",
    "text": "Bayesian analysis is just counting\n\nBayesian analysis counts all ways that something can happen (according to assumptions/model). Assumptions with more ways that are consistent with data are more plausible.\nThis method is not demonstrably different than standard approaches. Standard likelihood approaches use the values that are most consistent with the data as an estimate. Try out all possible numbers and then tells you which one is most likely.\nWhere Bayes differs, is we will focus beyond just a “best estimate”"
  },
  {
    "objectID": "10-bayes.html#visualizing-uncertainty",
    "href": "10-bayes.html#visualizing-uncertainty",
    "title": "Bayes",
    "section": "Visualizing uncertainty",
    "text": "Visualizing uncertainty\nOur posterior (ie different educated guesses at a the correct parameters; distribution of plausible values) is highlighting: that there is no ONE result, that there are many possible results that are consistent with the data.\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()"
  },
  {
    "objectID": "10-bayes.html#some-positives-of-focusing-on-uncertainty",
    "href": "10-bayes.html#some-positives-of-focusing-on-uncertainty",
    "title": "Bayes",
    "section": "Some positives of focusing on uncertainty",
    "text": "Some positives of focusing on uncertainty\n\nDo not need to assume normal or multivariate normal. Uncertainty does not need to be even tailed.\nDifferences (say across groups) in uncertainty is allowed. Do not need to assume groups have same standard errors. One can better account for and/or probe situations where a certain group has a lot or little variability.\nEasy to calculate uncertainty"
  },
  {
    "objectID": "10-bayes.html#cis-around-a-particular-value",
    "href": "10-bayes.html#cis-around-a-particular-value",
    "title": "Bayes",
    "section": "CIs around a particular value",
    "text": "CIs around a particular value\nWith your current knowledge, calculate a 95% CI around parent = 72 inches, to tell you what is possible for the sample mean at that hight.\n\\[  \\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{\\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}  \\]"
  },
  {
    "objectID": "10-bayes.html#be-comfortable-integrating-prior-knowledge",
    "href": "10-bayes.html#be-comfortable-integrating-prior-knowledge",
    "title": "Bayes",
    "section": "3. Be comfortable integrating prior knowledge",
    "text": "3. Be comfortable integrating prior knowledge\n\nPriors insert knowledge you have outside of your data into your model\nThis can seem “subjective” as opposed to the more “objective” way of letting the data speak.\n\n\n\nWe will mostly not “tip the scales” towards an outcome we want.\nMost of the time the prior knowledge constrains plausible or implausible range of values e.g. we know an effect size of a million is very unlikely.\nOften priors don’t matter…"
  },
  {
    "objectID": "10-bayes.html#okay-so-what-does-this-mean",
    "href": "10-bayes.html#okay-so-what-does-this-mean",
    "title": "Bayes",
    "section": "Okay so what does this mean?",
    "text": "Okay so what does this mean?\nIt means, BEFORE WE SEE THE DATA we are comfortable with different regression lines."
  },
  {
    "objectID": "10-bayes.html#okay-so-why-is-this-important",
    "href": "10-bayes.html#okay-so-why-is-this-important",
    "title": "Bayes",
    "section": "Okay so why is this important?",
    "text": "Okay so why is this important?\n\nA model that makes impossible predictions prior to seeing the data isn’t too useful. Why waste the effort? We often know what values are likely, given what we know about effect sizes\nThis is exactly what we do with standard “frequentist” methods. They have implicit priors such that all values, from negative infinity to positive infinity are equally likely.\nIf we use priors from a uniform distribution we will get the EXACT same results as a frequentist method."
  },
  {
    "objectID": "10-bayes.html#tying-it-together",
    "href": "10-bayes.html#tying-it-together",
    "title": "Bayes",
    "section": "Tying it together",
    "text": "Tying it together\n\nBe comfortable with a different estimation algorithm\nThink of results in terms of distributions\nBe comfortable integrating prior knowledge\n\n\\[p(\\theta | data) \\propto \\frac{p(data | \\theta) \\times p(\\theta )}{p(data)}\\] P(θ|data) is the posterior probability.\nP(θ) is the prior probability.\np(data| \\(\\theta\\) ) is the likelihood.\np(data) can be ignored, it is just a normalized coefficient"
  },
  {
    "objectID": "10-bayes.html#combining-the-three-components",
    "href": "10-bayes.html#combining-the-three-components",
    "title": "Bayes",
    "section": "Combining the three components",
    "text": "Combining the three components\nPriors influencing Posterior"
  },
  {
    "objectID": "10-bayes.html#sample-size-influence",
    "href": "10-bayes.html#sample-size-influence",
    "title": "Bayes",
    "section": "sample size influence",
    "text": "sample size influence"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior",
    "href": "10-bayes.html#going-from-prior-to-posterior",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\nWhat is our regression estimate again?\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nselect(b_parent) %&gt;% \n  mode_hdi(.width = c(.95))\n\n\n# A tibble: 1 × 6\n  b_parent .lower .upper .width .point .interval\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    0.652  0.562  0.724   0.95 mode   hdi"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior-1",
    "href": "10-bayes.html#going-from-prior-to-posterior-1",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\nWith a prior for b0 of N(0, .5)\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \n  ggplot(aes(x = b_parent)) +\n  stat_slab() +\n  stat_function(data = data.frame(x = c(-2, 2)), aes(x), fun = dnorm, n = 100, args = list(0, .5))"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior-2",
    "href": "10-bayes.html#going-from-prior-to-posterior-2",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\n\nplusible lines prior to data –&gt; plausible lines after data"
  },
  {
    "objectID": "10-bayes.html#what-is-confusing",
    "href": "10-bayes.html#what-is-confusing",
    "title": "Bayes",
    "section": "What is confusing:",
    "text": "What is confusing:\n\n\nIs it a philosophical different frame work? We can talk about how it is p(H0|d) vs p(d|H0) but it really doesn’t matter. Become a Bayesian just means using the algorithm, and again, most of us don’t have strong algo preferences\nTechnically we don’t have P-values, but Bayesian has analogues. Technically there isn’t NHST (because no null distribution to create sampling distribution) but you can easily do it.\n\nWhy don’t we do this already? Isn’t frequentist better? Historical accident due to computation limitations\nBayes Factors. Mostly garbage (imho) as they can be easily manipulated. But they have their place. BFs =/= Bayesian."
  },
  {
    "objectID": "10-bayes.html#first-bayes-example",
    "href": "10-bayes.html#first-bayes-example",
    "title": "Bayes",
    "section": "First bayes example",
    "text": "First bayes example\n\n\nCode\nlibrary(tidyverse)\ngalton.data &lt;- psychTools::galton"
  },
  {
    "objectID": "10-bayes.html#model-we-want-to-fit",
    "href": "10-bayes.html#model-we-want-to-fit",
    "title": "Bayes",
    "section": "model we want to fit",
    "text": "model we want to fit\n\\[ child_i \\sim Normal( \\mu_i , \\sigma )\\ \\]\n\\[\\mu_i = \\beta_0 + \\beta_1  parent_i \\]"
  },
  {
    "objectID": "10-bayes.html#regression-with-brms",
    "href": "10-bayes.html#regression-with-brms",
    "title": "Bayes",
    "section": "regression with brms",
    "text": "regression with brms\n\n\nCode\nlibrary(brms)\nfit.1.bayesian &lt;- brm(child ~ parent, data = galton.data,\n                      backend = \"cmdstanr\",\n                      file = \"fit.1.b\")"
  },
  {
    "objectID": "10-bayes.html#prior-for-intercept",
    "href": "10-bayes.html#prior-for-intercept",
    "title": "Bayes",
    "section": "Prior for intercept",
    "text": "Prior for intercept\n\n\nCode\n  tibble(x = seq(from = 0, to = 100, by = .1)) %&gt;% \n  ggplot(aes(x = x, y = dnorm(x, mean = 68, sd = 5))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +\n  labs(title = \"mu ~ dnorm(68, 5)\",\n       y = \"density\")"
  },
  {
    "objectID": "10-bayes.html#prior-for-regression-coefficent",
    "href": "10-bayes.html#prior-for-regression-coefficent",
    "title": "Bayes",
    "section": "Prior for regression coefficent",
    "text": "Prior for regression coefficent\n\n\nCode\n  tibble(x = seq(from = -15, to = 15, by = .1)) %&gt;% \n  ggplot(aes(x = x, y = dnorm(x, mean = 0, sd = 5))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = -15, to = 15, by = 3)) +\n  labs(title = \"mu ~ dnorm(0, 5)\",\n       y = \"density\")"
  },
  {
    "objectID": "10-bayes.html#prior-for-sigma",
    "href": "10-bayes.html#prior-for-sigma",
    "title": "Bayes",
    "section": "Prior for sigma",
    "text": "Prior for sigma\n\n\nCode\np.s &lt;- ggplot(data.frame(x = c(0, 10)), aes(x)) +\n  stat_function(fun = dcauchy, n = 200, args = list(0, 1)) +\n  labs(title = \"sigma ~ HalfCauchy(0,1)\")\np.s\n\n\n\n\nWe know that variances are going to be positive.\nWhat is an upper bound possibility?"
  },
  {
    "objectID": "10-bayes.html#what-is-in-the-brms-object",
    "href": "10-bayes.html#what-is-in-the-brms-object",
    "title": "Bayes",
    "section": "What is in the brms object?",
    "text": "What is in the brms object?\nThe posterior!\n\n\nCode\nas_draws(fit.2)\n\n\n# A draws_list: 500 iterations, 2 chains, and 6 variables\n\n[chain = 1]\n$b_Intercept\n [1] 68 68 68 68 68 68 68 68 68 68\n\n$b_parent.c\n [1] 0.65 0.63 0.70 0.62 0.67 0.67 0.66 0.72 0.58 0.67\n\n$sigma\n [1] 2.2 2.2 2.2 2.3 2.2 2.3 2.2 2.3 2.1 2.4\n\n$Intercept\n [1] 68 68 68 68 68 68 68 68 68 68\n\n\n[chain = 2]\n$b_Intercept\n [1] 68 68 68 68 68 68 68 68 68 68\n\n$b_parent.c\n [1] 0.70 0.69 0.67 0.66 0.62 0.65 0.69 0.61 0.71 0.71\n\n$sigma\n [1] 2.2 2.2 2.2 2.3 2.3 2.2 2.3 2.3 2.2 2.2\n\n$Intercept\n [1] 68 68 68 68 68 68 68 68 68 68\n\n# ... with 490 more iterations, and 2 more variables"
  },
  {
    "objectID": "10-bayes.html#what-is-in-the-brms-object-1",
    "href": "10-bayes.html#what-is-in-the-brms-object-1",
    "title": "Bayes",
    "section": "What is in the brms object?",
    "text": "What is in the brms object?\n\n\nCode\nlibrary(tidybayes)\nget_variables(fit.2)\n\n\n [1] \"b_Intercept\"   \"b_parent.c\"    \"sigma\"         \"Intercept\"    \n [5] \"lprior\"        \"lp__\"          \"accept_stat__\" \"treedepth__\"  \n [9] \"stepsize__\"    \"divergent__\"   \"n_leapfrog__\"  \"energy__\""
  },
  {
    "objectID": "10-bayes.html#prior-and-posterior-plotted-together",
    "href": "10-bayes.html#prior-and-posterior-plotted-together",
    "title": "Bayes",
    "section": "prior and posterior plotted together",
    "text": "prior and posterior plotted together\n\n\nCode\nfit.2 %&gt;% \n  spread_draws(b_parent.c) %&gt;% \n  ggplot(aes(x = b_parent.c)) +\n  stat_slab() +\n  stat_function(data = data.frame(x = c(-10, 10)), aes(x), fun = dnorm, n = 100, args = list(0, 5))"
  },
  {
    "objectID": "10-bayes.html#predictedfitted-values",
    "href": "10-bayes.html#predictedfitted-values",
    "title": "Bayes",
    "section": "Predicted/fitted values",
    "text": "Predicted/fitted values\n\n\nIf we examine a expected/predicted mean at a certain value across all of our samples we directly compute our uncertainty. In contrast to frequentist where we have to use an ugly equation, which has big assumptions.\n\n\n\nCode\nfit.2 %&gt;% \n  spread_draws(b_Intercept,b_parent.c) %&gt;% \n  select(b_Intercept,b_parent.c) %&gt;% \n  mutate(mu_at_64 = b_Intercept + (b_parent.c * -4.3))\n\n\n# A tibble: 1,000 × 3\n   b_Intercept b_parent.c mu_at_64\n         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1        68.1      0.646     65.4\n 2        68.0      0.626     65.3\n 3        68.1      0.700     65.0\n 4        68.1      0.618     65.5\n 5        68.1      0.674     65.2\n 6        68.0      0.670     65.1\n 7        68.0      0.658     65.2\n 8        68.0      0.715     64.9\n 9        68.2      0.581     65.7\n10        68.1      0.666     65.2\n# ℹ 990 more rows"
  },
  {
    "objectID": "10-bayes.html#predictedfitted-values-1",
    "href": "10-bayes.html#predictedfitted-values-1",
    "title": "Bayes",
    "section": "Predicted/fitted values",
    "text": "Predicted/fitted values\nWe can calculate not only the mean but also the dispersion. In lm land we had to use a funky equation to calculate the CI around some predicted value of X. Now we can use samples. It is just counting up where the xx% of samples fall.\n\n\nCode\nfit.2 %&gt;% \n  spread_draws(b_Intercept,b_parent.c) %&gt;% \n  select(b_Intercept,b_parent.c) %&gt;% \n  mutate(mu_at_64 = b_Intercept + (b_parent.c * -4.3)) %&gt;% \n  ggplot(aes(x = mu_at_64)) +\n  stat_slab() +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(mu[\"Child-height at parent 64\"]))"
  },
  {
    "objectID": "10-bayes.html#predictedfitted-values-2",
    "href": "10-bayes.html#predictedfitted-values-2",
    "title": "Bayes",
    "section": "Predicted/fitted values",
    "text": "Predicted/fitted values\n\nWhat did we do? We calculated the value of our DV when our predictor = -4.3 units under the mean (64 inches). (Basically a slice of the regression band from a few slides ago)\nHow did we do it? We fed our model an X that we were interested in to calculate a Y-hat.\nWe will be using this idea A. LOT. This isn’t Bayesian specific. Instead it is a way you should think about all models. It is often VERY useful to use (e.g., getting group values when using dummy’s, testing contrasts, etc)."
  },
  {
    "objectID": "2-hw.html",
    "href": "2-hw.html",
    "title": "Homework 2",
    "section": "",
    "text": "We will use the `palmerpenguins` dataset. Please install the pacakge `palmerpenguins` if you do not already have it and use the `penguins` data.frame.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .qmd/.Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "2-hw.html#question-1",
    "href": "2-hw.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nRun a model to test the question:\n&gt; Do male and female penguins have different length flippers?\n\nYes or no, is the flipper length of male penguins signiciantly different from the flipper length of female penguins? How do you know?\nWhat is the (mean) flipper length for males and females, respectively?\nIs this a good model? How do you know?\nUsing the output, write the formal equation (see Slide 26 if confused)\nInterpret the intercept and the regression coefficient\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nMake a figure to illustrate this relationship. In it, you should include a summary metric (e.g., the mean or median) as well as information about the distribution of data, the raw data, or both. Look at slides for inspiration!"
  },
  {
    "objectID": "2-hw.html#question-2",
    "href": "2-hw.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nRun a model to test the question:\n&gt; Are penguins on different islands the same size?\n\nTry to write the formal equation – just do your best! In a sentence or two, what makes this equation different from the one you wrote in Question 1?\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nGiven the name of this statistical test, replace `lm` with a different function. That is, use a function that is *not* `lm` to run the same model. (I’m being intentionally vague here…). What changes about this new output, compared to the model in Question 1, and what stays the same?\nMake a figure to compare the size of penguins on different islands. Make sure to give an interval around the point estimate (*hint: this usually comes in the form of ‘bars’*)\nIs this a good model? How do you know?\nGiven the name of this statistical test, calculate an effect size. What do you notice between this effect size and the `lm` object output? Any similarities?"
  },
  {
    "objectID": "3-Basics.html#this-time",
    "href": "3-Basics.html#this-time",
    "title": "GLM basics II",
    "section": "This time",
    "text": "This time\n\nWhat is regression and why is it useful?\nNuts and bolts\n\nEquation\nOrdinary least squares\nInterpretation"
  },
  {
    "objectID": "3-Basics.html#regression",
    "href": "3-Basics.html#regression",
    "title": "GLM basics II",
    "section": "Regression",
    "text": "Regression\nRegression is a general data analytic system, meaning lots of things fall under the umbrella of regression. This system can handle a variety of forms of relations, although all forms have to be specified in a linear way. Usefully, we can incorporate IVs of all nature – continuous, categorical, nominal, ordinal….\nThe output of regression includes both effect sizes and, if using frequentist or Bayesian software, statistical significance. We can also incorporate multiple influences (IVs) and account for their intercorrelations."
  },
  {
    "objectID": "3-Basics.html#regression-1",
    "href": "3-Basics.html#regression-1",
    "title": "GLM basics II",
    "section": "Regression",
    "text": "Regression\n\nScientific use: explaining the influence of one or more variables on some outcome.\n\nDoes this intervention affect reaction time? Does self-esteem predict relationship quality?\n\nPrediction use: We can develop models based on what’s happened in the past to predict what will happen in the figure.\n\nInsurance premiums? Graduate school… success?\n\nAdjustment: Statistically control for known effects\n\nIf everyone had the same level of SES, would abuse still be associated with criminal behavior?"
  },
  {
    "objectID": "3-Basics.html#regression-equation",
    "href": "3-Basics.html#regression-equation",
    "title": "GLM basics II",
    "section": "Regression equation",
    "text": "Regression equation\nWhat is a regression equation?\n\nFunctional relationship\n\nIdeally like a physical law \\((E = MC^2)\\)\nIn practice, it’s never as robust as that\n\n\nHow do we uncover the relationship?"
  },
  {
    "objectID": "3-Basics.html#how-does-y-vary-with-x",
    "href": "3-Basics.html#how-does-y-vary-with-x",
    "title": "GLM basics II",
    "section": "How does Y vary with X?",
    "text": "How does Y vary with X?\n\nThe regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X\n“Our best guess” regardless of whether our model includes categories or continuous predictor variables\nWe will evaluate our guesses based on how far away we are from the mean. But how do we come up with those guesses in the first place?"
  },
  {
    "objectID": "3-Basics.html#regression-equation-1",
    "href": "3-Basics.html#regression-equation-1",
    "title": "GLM basics II",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[\\Large Y = b_{0} + b_{1}X +e\\]\n\\[\\Large \\widehat{Y} = b_{0} + b_{1}X\\]\n\\(\\widehat{Y}\\) signifies the predicted score – no error\nThe difference between the predicted and observed score is the residual ( \\(e_i\\) )"
  },
  {
    "objectID": "3-Basics.html#ols",
    "href": "3-Basics.html#ols",
    "title": "GLM basics II",
    "section": "OLS",
    "text": "OLS\n\nHow do we find the regression estimates?\nOrdinary Least Squares (OLS) estimation\nMinimizes deviations\n\n\\[ min\\sum(Y_{i}-\\widehat{Y})^{2} \\]\n\nOther estimation procedures possible (and necessary in some cases)"
  },
  {
    "objectID": "3-Basics.html#compare-with-bad-fit",
    "href": "3-Basics.html#compare-with-bad-fit",
    "title": "GLM basics II",
    "section": "compare with bad fit",
    "text": "compare with bad fit\n\n\nCode\nnew.i = 1.1\nnew.slope = -0.7\nd1.f$new.fitted = 1.1 -0.7*d1.f$x.1\n\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_abline(intercept = new.i, slope = new.slope, color = \"blue\", linewidth = 1) +\n  geom_point(aes(y = new.fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = new.fitted))+\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "3-Basics.html#what-is-error",
    "href": "3-Basics.html#what-is-error",
    "title": "GLM basics II",
    "section": "What is error?",
    "text": "What is error?\n\\[\\Large Y = b_{0} + b_{1}X +e\\]\n\\[\\Large \\hat{Y} = b_{0} + b_{1}X\\]\n\\[\\Large Y_i = \\hat{Y_i} + e_i\\]\n\\[\\Large e_i = Y_i - \\hat{Y_i}\\]"
  },
  {
    "objectID": "3-Basics.html#ols-1",
    "href": "3-Basics.html#ols-1",
    "title": "GLM basics II",
    "section": "OLS",
    "text": "OLS\nThe line that yields the smallest sum of squared deviations\n\\[\\Large \\Sigma(Y_i - \\hat{Y_i})^2\\] \\[\\Large = \\Sigma(Y_i - (b_0+b_{1}X_i))^2\\] \\[\\Large = \\Sigma(e_i)^2\\]"
  },
  {
    "objectID": "3-Basics.html#regression-coefficient-b_1",
    "href": "3-Basics.html#regression-coefficient-b_1",
    "title": "GLM basics II",
    "section": "Regression coefficient, \\(b_{1}\\)",
    "text": "Regression coefficient, \\(b_{1}\\)\n\\[\\large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}\\]\n\\[\\large r_{xy} = \\frac{s_{xy}}{s_xs_y}\\]\n\nThe regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X"
  },
  {
    "objectID": "3-Basics.html#standardized-regression",
    "href": "3-Basics.html#standardized-regression",
    "title": "GLM basics II",
    "section": "Standardized regression",
    "text": "Standardized regression\n\nRegression using z-scores for Y and X\nCorrelation equals standardized regression coefficient\n\n\\[\\large b_{1} = r_{xy} \\frac{s_{y}}{s_{x}}\\]\n\\[ \\large r_{xy} = b_1\\frac{s_x}{s_y} \\]\nIf the variance of both X and Y is equal to 1 (as in z-scores):\n\\[\\large \\beta_{1} = b_{1}^* = r_{xy}\\]"
  },
  {
    "objectID": "3-Basics.html#standardized-regression-equation",
    "href": "3-Basics.html#standardized-regression-equation",
    "title": "GLM basics II",
    "section": "Standardized regression equation",
    "text": "Standardized regression equation\n\\[\\large Y = b_{1}^*X+e\\]\n\\[\\large b_{1}^* = b_{1}\\frac{s_x}{s_y}\\]\nWhen \\(X = 0, Y = 0\\). A one-standard deviation increase in X is associated with a \\(b_{1}^*\\) standard deviation increase in Y. Our regression coefficient is equivalent to the correlation coefficient when we have only one predictor in our model."
  },
  {
    "objectID": "3-Basics.html#estimating-the-intercept-b_0",
    "href": "3-Basics.html#estimating-the-intercept-b_0",
    "title": "GLM basics II",
    "section": "Estimating the intercept, \\(b_0\\)",
    "text": "Estimating the intercept, \\(b_0\\)\n\nRe-write equation to include the means of Y and X ( \\(\\bar{X}\\) & \\(\\bar{Y}\\) )\nIntercept serves to adjust for differences in means between X and Y\n\n\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\] - If standardized, intercept drops out. Otherwise, intercept is where regression line crosses the y-axis at X = 0\n- Notice that when \\(X = \\bar{X}\\) the regression line goes through \\(\\bar{Y}\\). This is true for all regressions such that the regression line must pass through \\(\\bar{X}\\) and \\(\\bar{Y}\\)"
  },
  {
    "objectID": "3-Basics.html#example",
    "href": "3-Basics.html#example",
    "title": "GLM basics II",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(psych)\ngalton.data &lt;- psychTools::galton\nhead(galton.data)\n\n\n  parent child\n1   70.5  61.7\n2   68.5  61.7\n3   65.5  61.7\n4   64.5  61.7\n5   64.0  61.7\n6   67.5  62.2\n\n\nCode\ndescribe(galton.data, fast = T)\n\n\n       vars   n  mean   sd median  min  max range  skew kurtosis   se\nparent    1 928 68.31 1.79   68.5 64.0 73.0     9 -0.04     0.05 0.06\nchild     2 928 68.09 2.52   68.2 61.7 73.7    12 -0.09    -0.35 0.08\n\n\nCode\ncor(galton.data)\n\n\n          parent     child\nparent 1.0000000 0.4587624\nchild  0.4587624 1.0000000"
  },
  {
    "objectID": "3-Basics.html#in-r",
    "href": "3-Basics.html#in-r",
    "title": "GLM basics II",
    "section": "In R",
    "text": "In R\n\n\nCode\nfit.1 &lt;- lm(child ~ parent, data = galton.data)\nsummary(fit.1)\n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "3-Basics.html#reversed",
    "href": "3-Basics.html#reversed",
    "title": "GLM basics II",
    "section": "Reversed",
    "text": "Reversed\n\n\nCode\nsummary(lm(parent ~ child, data = galton.data))\n\n\n\nCall:\nlm(formula = parent ~ child, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6702 -1.1702 -0.1471  1.1324  4.2722 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***\nchild        0.32565    0.02073   15.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.589 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "3-Basics.html#data-predicted-and-residuals",
    "href": "3-Basics.html#data-predicted-and-residuals",
    "title": "GLM basics II",
    "section": "Data, predicted, and residuals",
    "text": "Data, predicted, and residuals\n\n\nCode\nlibrary(broom)\nmodel_info = augment(fit.1)\nhead(model_info)\n\n\n# A tibble: 6 × 8\n  child parent .fitted .resid    .hat .sigma .cooksd .std.resid\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1  61.7   70.5    69.5  -7.81 0.00270   2.22 0.0165       -3.49\n2  61.7   68.5    68.2  -6.51 0.00109   2.23 0.00462      -2.91\n3  61.7   65.5    66.3  -4.57 0.00374   2.23 0.00787      -2.05\n4  61.7   64.5    65.6  -3.93 0.00597   2.24 0.00931      -1.76\n5  61.7   64      65.3  -3.60 0.00735   2.24 0.00966      -1.62\n6  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40"
  },
  {
    "objectID": "3-Basics.html#residuals",
    "href": "3-Basics.html#residuals",
    "title": "GLM basics II",
    "section": "Residuals",
    "text": "Residuals\n\nDispersion of residuals can be thought of as what is left over in Y that is not explained by our model. As residuals get smaller on average, so will the SD of the residuals.\nSigma ( \\(\\sigma\\) ) is the SD of residuals. It can be thought of as how much left over in Y that we cannot explain by our model."
  },
  {
    "objectID": "3-Basics.html#residuals-summary",
    "href": "3-Basics.html#residuals-summary",
    "title": "GLM basics II",
    "section": "Residuals Summary",
    "text": "Residuals Summary\n\nResiduals are not correlated with \\(X\\) and \\(\\hat{Y}\\) because those two are perfectly correlated with one another (that is, \\(r_{\\text{fitted,x}} = 1\\) )\n\\(X\\) and \\(\\hat{Y}\\) represent the same information. We use our model ( \\(X\\) ) to make a prediction ( \\(\\hat{Y}\\) ). These predictions are entirely based on the model.\nThere is no correlation between residuals with \\(X\\) and \\(\\hat{Y}\\) because they are created by subtracting them out of \\(Y\\). That is, ( \\(\\epsilon = Y - \\hat{Y}\\) )\nSigma ( \\(\\sigma\\) ; the SD of residuals) can be thought of as how much left over in \\(Y\\) after we take out all of the information our model provides."
  }
]